//
//  VisionAnalyzer.swift
//  Project_Color
//
//  Created by AI Assistant on 2025/11/18.
//  Vision æ¡†æ¶é›†æˆï¼šåœºæ™¯è¯†åˆ«ã€æ˜¾è‘—æ€§åˆ†æã€å›¾åƒåˆ†ç±»ã€åœ°å¹³çº¿æ£€æµ‹
//

import Foundation
import Vision
#if canImport(UIKit)
import UIKit
#endif
#if canImport(CoreImage)
import CoreImage
#endif

class VisionAnalyzer {
    #if canImport(UIKit)
    private let ciContext = CIContext()
    #endif
    
    // MARK: - ä¸»åˆ†ææ–¹æ³•
    
    /// å¯¹å›¾ç‰‡è¿›è¡Œå®Œæ•´çš„ Vision åˆ†æ
    /// - Parameter image: UIImage å¯¹è±¡
    /// - Returns: PhotoVisionInfo åŒ…å«æ‰€æœ‰è¯†åˆ«ç»“æœ
    func analyzeImage(_ image: UIImage) async -> PhotoVisionInfo? {
        #if canImport(UIKit)
        guard let cgImage = makeCGImage(from: image) else {
            print("âŒ Vision: æ— æ³•è·å– CGImage")
            return nil
        }
        
        print("\nğŸ” Vision åˆ†æå¼€å§‹...")
        print("   å›¾ç‰‡å°ºå¯¸: \(cgImage.width) x \(cgImage.height)")
        print("   è‰²å½©ç©ºé—´: \(cgImage.colorSpace?.name ?? "unknown" as CFString)")
        
        // æ£€æµ‹æ˜¯å¦åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œ
        #if targetEnvironment(simulator)
        print("   âš ï¸ è¿è¡Œåœ¨æ¨¡æ‹Ÿå™¨ä¸Š - æŸäº› Vision åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        #else
        print("   âœ… è¿è¡Œåœ¨çœŸæœºä¸Š")
        #endif
        
        // åˆ›å»ºè¯·æ±‚å¤„ç†å™¨
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        // å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†æï¼Œæ”¶é›†ç»“æœ
        let (scenes, saliency, objects, horizon) = await withTaskGroup(
            of: VisionAnalysisResult.self,
            returning: (
                [SceneClassification],
                [SaliencyObject],
                [RecognizedObject],
                (Float?, String?)?
            ).self
        ) { group in
            // åœºæ™¯è¯†åˆ«ï¼ˆVNClassifyImageRequest è¿”å›åœºæ™¯åˆ†ç±»ï¼‰
            group.addTask {
                await self.performSceneClassification(handler: handler)
            }
            
            // æ˜¾è‘—æ€§åˆ†æï¼ˆä¸»ä½“ä½ç½®ï¼‰
            group.addTask {
                await self.performSaliencyAnalysis(handler: handler)
            }
            
            // å¯¹è±¡æ£€æµ‹ï¼ˆåŠ¨ç‰© + äººä½“ï¼‰
            group.addTask {
                await self.performObjectRecognition(handler: handler)
            }
            
            // åœ°å¹³çº¿æ£€æµ‹
            group.addTask {
                await self.performHorizonDetection(handler: handler)
            }
            
            // æ”¶é›†ç»“æœ
            var scenes: [SceneClassification] = []
            var saliency: [SaliencyObject] = []
            var objects: [RecognizedObject] = []
            var horizon: (Float?, String?)? = nil
            
            for await result in group {
                switch result {
                case .sceneClassifications(let items):
                    scenes = items
                case .saliencyObjects(let items):
                    saliency = items
                case .imageClassifications(let items):
                    // ç§»é™¤äº†é‡å¤çš„å›¾åƒåˆ†ç±»ï¼ŒVNClassifyImageRequest å°±æ˜¯åœºæ™¯åˆ†ç±»
                    break
                case .recognizedObjects(let items):
                    objects = items
                case .horizonDetection(let angle, let transform):
                    horizon = (angle, transform)
                }
            }
            
            return (scenes, saliency, objects, horizon)
        }
        
        // æ„å»º visionInfo
        var visionInfo = PhotoVisionInfo()
        visionInfo.sceneClassifications = scenes
        visionInfo.saliencyObjects = saliency
        visionInfo.imageClassifications = []  // ä¸å†ä½¿ç”¨ï¼Œé¿å…é‡å¤
        visionInfo.recognizedObjects = objects
        visionInfo.horizonAngle = horizon?.0
        visionInfo.horizonTransform = horizon?.1
        
        // æ¨æ–­æ‘„å½±å±æ€§
        visionInfo.photographyAttributes = inferPhotographyAttributes(from: visionInfo)
        
        // æ‰“å°å®Œæ•´ç»“æœ
        printVisionResults(visionInfo)
        
        print("âœ… Vision åˆ†æå®Œæˆ\n")
        
        return visionInfo
        #else
        return nil
        #endif
    }

    #if canImport(UIKit)
    private func makeCGImage(from image: UIImage) -> CGImage? {
        if let cgImage = image.cgImage {
            return cgImage
        }
        if let ciImage = image.ciImage {
            return ciContext.createCGImage(ciImage, from: ciImage.extent)
        }
        let format = UIGraphicsImageRendererFormat.default()
        format.scale = image.scale
        let renderer = UIGraphicsImageRenderer(size: image.size, format: format)
        let renderedImage = renderer.image { _ in
            image.draw(at: .zero)
        }
        return renderedImage.cgImage
    }
    #endif

    // MARK: - åˆ†æç»“æœæšä¸¾
    
    private enum VisionAnalysisResult {
        case sceneClassifications([SceneClassification])
        case saliencyObjects([SaliencyObject])
        case imageClassifications([ImageClassification])
        case recognizedObjects([RecognizedObject])
        case horizonDetection(angle: Float?, transform: String?)
    }
    
    // MARK: - åœºæ™¯è¯†åˆ«
    
    private func performSceneClassification(handler: VNImageRequestHandler) async -> VisionAnalysisResult {
        // ä½¿ç”¨å›è°ƒæ–¹å¼çš„è¯·æ±‚ï¼Œæ›´ç¨³å®š
        var resultObservations: [VNClassificationObservation] = []
        var requestError: Error?
        
        let request = VNClassifyImageRequest { request, error in
            if let error = error {
                requestError = error
                return
            }
            
            if let observations = request.results as? [VNClassificationObservation] {
                resultObservations = observations
            }
        }
        
        // è®¾ç½®è¯·æ±‚é€‰é¡¹
        request.usesCPUOnly = false  // å…è®¸ä½¿ç”¨ GPU/Neural Engine
        
        do {
            try handler.perform([request])
            
            // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if let error = requestError {
                print("âŒ Vision: åœºæ™¯è¯†åˆ«å›è°ƒé”™è¯¯ - \(error.localizedDescription)")
                return .sceneClassifications([])
            }
            
            if resultObservations.count > 0 {
                print("ğŸ” åœºæ™¯è¯†åˆ«: è·å–åˆ° \(resultObservations.count) ä¸ªç»“æœ")
                
                // æ‰“å°å‰5ä¸ªç»“æœ
                print("   å‰5ä¸ªç»“æœ:")
                for (i, obs) in resultObservations.prefix(5).enumerated() {
                    print("      \(i+1). \(obs.identifier): \(String(format: "%.3f", obs.confidence))")
                }
                
                // ä¿ç•™ç½®ä¿¡åº¦ > 0.05 çš„ç»“æœï¼Œæœ€å¤š10ä¸ª
                let filtered = resultObservations
                    .filter { $0.confidence > 0.05 }
                    .prefix(10)
                
                print("   - è¿‡æ»¤å (>0.05): \(filtered.count) ä¸ªç»“æœ")
                
                let results = filtered.map { obs in
                    SceneClassification(
                        identifier: obs.identifier,
                        confidence: obs.confidence
                    )
                }
                return .sceneClassifications(results)
            } else {
                #if targetEnvironment(simulator)
                print("âš ï¸ Vision: åœºæ™¯è¯†åˆ«è¿”å›ç©ºç»“æœ (å¯èƒ½æ˜¯æ¨¡æ‹Ÿå™¨é™åˆ¶)")
                #else
                print("âš ï¸ Vision: åœºæ™¯è¯†åˆ«è¿”å›ç©ºç»“æœ")
                #endif
            }
        } catch {
            print("âŒ Vision: åœºæ™¯è¯†åˆ«æ‰§è¡Œå¤±è´¥ - \(error.localizedDescription)")
            print("   é”™è¯¯è¯¦æƒ…: \(error)")
        }
        
        return .sceneClassifications([])
    }
    
    // MARK: - æ˜¾è‘—æ€§åˆ†æï¼ˆä¸»ä½“ä½ç½®ï¼‰
    
    private func performSaliencyAnalysis(handler: VNImageRequestHandler) async -> VisionAnalysisResult {
        // ä½¿ç”¨åŸºäºå¯¹è±¡çš„æ˜¾è‘—æ€§åˆ†æï¼ˆå›è°ƒæ–¹å¼ï¼‰
        var resultObservation: VNSaliencyImageObservation?
        var requestError: Error?
        
        let request = VNGenerateObjectnessBasedSaliencyImageRequest { request, error in
            if let error = error {
                requestError = error
                return
            }
            
            if let observation = request.results?.first as? VNSaliencyImageObservation {
                resultObservation = observation
            }
        }
        
        do {
            try handler.perform([request])
            
            // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if let error = requestError {
                print("âŒ Vision: æ˜¾è‘—æ€§åˆ†æå›è°ƒé”™è¯¯ - \(error.localizedDescription)")
                return .saliencyObjects([])
            }
            
            if let observation = resultObservation {
                print("ğŸ” æ˜¾è‘—æ€§åˆ†æ: è·å–åˆ°è§‚å¯Ÿç»“æœ")
                // è·å–æ˜¾è‘—æ€§å¯¹è±¡
                if let objects = observation.salientObjects {
                    print("   - æ£€æµ‹åˆ° \(objects.count) ä¸ªæ˜¾è‘—å¯¹è±¡")
                    let results = objects.map { obj in
                        SaliencyObject(
                            boundingBox: obj.boundingBox,
                            confidence: obj.confidence
                        )
                    }
                    return .saliencyObjects(results)
                } else {
                    print("   - æœªæ£€æµ‹åˆ°æ˜¾è‘—å¯¹è±¡")
                }
            } else {
                #if targetEnvironment(simulator)
                print("âš ï¸ Vision: æ˜¾è‘—æ€§åˆ†æè¿”å›ç©ºç»“æœ (å¯èƒ½æ˜¯æ¨¡æ‹Ÿå™¨é™åˆ¶)")
                #else
                print("âš ï¸ Vision: æ˜¾è‘—æ€§åˆ†æè¿”å›ç©ºç»“æœ")
                #endif
            }
        } catch {
            print("âŒ Vision: æ˜¾è‘—æ€§åˆ†ææ‰§è¡Œå¤±è´¥ - \(error.localizedDescription)")
            print("   é”™è¯¯è¯¦æƒ…: \(error)")
        }
        
        return .saliencyObjects([])
    }
    
    // MARK: - å›¾åƒåˆ†ç±»ï¼ˆå·²å¼ƒç”¨ï¼‰
    // æ³¨æ„ï¼šVNClassifyImageRequest è¿”å›çš„å°±æ˜¯åœºæ™¯åˆ†ç±»ï¼Œä¸ performSceneClassification é‡å¤
    // å¦‚éœ€çœŸæ­£çš„å›¾åƒåˆ†ç±»ï¼ˆå¦‚è¯†åˆ«ç‰©ä½“ç±»åˆ«ï¼‰ï¼Œéœ€è¦ä½¿ç”¨è‡ªå®šä¹‰ Core ML æ¨¡å‹
    
    /*
    private func performImageClassification(handler: VNImageRequestHandler) async -> VisionAnalysisResult {
        // ä½¿ç”¨å›è°ƒæ–¹å¼çš„è¯·æ±‚
        var resultObservations: [VNClassificationObservation] = []
        var requestError: Error?
        
        let request = VNClassifyImageRequest { request, error in
            if let error = error {
                requestError = error
                return
            }
            
            if let observations = request.results as? [VNClassificationObservation] {
                resultObservations = observations
            }
        }
        
        // è®¾ç½®è¯·æ±‚é€‰é¡¹
        request.usesCPUOnly = false
        
        do {
            try handler.perform([request])
            
            // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if let error = requestError {
                print("âŒ Vision: å›¾åƒåˆ†ç±»å›è°ƒé”™è¯¯ - \(error.localizedDescription)")
                return .imageClassifications([])
            }
            
            if resultObservations.count > 0 {
                print("ğŸ” å›¾åƒåˆ†ç±»: è·å–åˆ° \(resultObservations.count) ä¸ªç»“æœ")
                
                // æ‰“å°å‰10ä¸ªç»“æœ
                print("   å‰10ä¸ªç»“æœ:")
                for (i, obs) in resultObservations.prefix(10).enumerated() {
                    print("      \(i+1). \(obs.identifier): \(String(format: "%.3f", obs.confidence))")
                }
                
                // ä¿ç•™ç½®ä¿¡åº¦ > 0.1 çš„åˆ†ç±»ï¼Œæœ€å¤š20ä¸ª
                let filtered = resultObservations
                    .filter { $0.confidence > 0.1 }
                    .prefix(20)
                
                print("   - è¿‡æ»¤å (>0.1): \(filtered.count) ä¸ªç»“æœ")
                
                let results = filtered.map { obs in
                    ImageClassification(
                        identifier: obs.identifier,
                        confidence: obs.confidence
                    )
                }
                return .imageClassifications(results)
            } else {
                #if targetEnvironment(simulator)
                print("âš ï¸ Vision: å›¾åƒåˆ†ç±»è¿”å›ç©ºç»“æœ (å¯èƒ½æ˜¯æ¨¡æ‹Ÿå™¨é™åˆ¶)")
                #else
                print("âš ï¸ Vision: å›¾åƒåˆ†ç±»è¿”å›ç©ºç»“æœ")
                #endif
            }
        } catch {
            print("âŒ Vision: å›¾åƒåˆ†ç±»æ‰§è¡Œå¤±è´¥ - \(error.localizedDescription)")
            print("   é”™è¯¯è¯¦æƒ…: \(error)")
        }
        
        return .imageClassifications([])
    }
    */
    
    // MARK: - å¯¹è±¡æ£€æµ‹
    
    private func performObjectRecognition(handler: VNImageRequestHandler) async -> VisionAnalysisResult {
        print("ğŸ” å¼€å§‹å¯¹è±¡æ£€æµ‹...")
        
        var allObjects: [RecognizedObject] = []
        
        // 1. åŠ¨ç‰©è¯†åˆ« (VNRecognizeAnimalsRequest)
        do {
            let animalRequest = VNRecognizeAnimalsRequest()
            animalRequest.usesCPUOnly = false
            
            try handler.perform([animalRequest])
            
            if let observations = animalRequest.results as? [VNRecognizedObjectObservation] {
                print("   ğŸ¾ åŠ¨ç‰©è¯†åˆ«: æ£€æµ‹åˆ° \(observations.count) ä¸ªåŠ¨ç‰©")
                for obs in observations {
                    if let label = obs.labels.first, label.confidence > 0.3 {
                        print("      - \(label.identifier): \(String(format: "%.3f", label.confidence))")
                        allObjects.append(RecognizedObject(
                            identifier: label.identifier,
                            confidence: label.confidence,
                            boundingBox: obs.boundingBox
                        ))
                    }
                }
            }
        } catch {
            print("   âš ï¸ åŠ¨ç‰©è¯†åˆ«å¤±è´¥: \(error.localizedDescription)")
        }
        
        // 2. äººä½“æ£€æµ‹ (VNDetectHumanRectanglesRequest)
        do {
            let humanRequest = VNDetectHumanRectanglesRequest()
            humanRequest.usesCPUOnly = false
            
            try handler.perform([humanRequest])
            
            if let observations = humanRequest.results as? [VNHumanObservation] {
                print("   ğŸ‘¤ äººä½“æ£€æµ‹: æ£€æµ‹åˆ° \(observations.count) ä¸ªäººä½“")
                for (index, obs) in observations.enumerated() {
                    print("      - person_\(index + 1): \(String(format: "%.3f", obs.confidence))")
                    allObjects.append(RecognizedObject(
                        identifier: "person",
                        confidence: obs.confidence,
                        boundingBox: obs.boundingBox
                    ))
                }
            }
        } catch {
            print("   âš ï¸ äººä½“æ£€æµ‹å¤±è´¥: \(error.localizedDescription)")
        }
        
        print("   âœ… å¯¹è±¡æ£€æµ‹å®Œæˆ: å…± \(allObjects.count) ä¸ªå¯¹è±¡")
        print("   â„¹ï¸ æ³¨æ„: Vision æ¡†æ¶ä»…æ”¯æŒåŠ¨ç‰©å’Œäººä½“æ£€æµ‹")
        print("   â„¹ï¸ å¦‚éœ€æ£€æµ‹æ›´å¤šç‰©ä½“(å¦‚å»ºç­‘ã€è½¦è¾†ç­‰)ï¼Œéœ€è¦è‡ªå®šä¹‰ Core ML æ¨¡å‹")
        
        return .recognizedObjects(allObjects)
    }
    
    // MARK: - åœ°å¹³çº¿æ£€æµ‹
    
    private func performHorizonDetection(handler: VNImageRequestHandler) async -> VisionAnalysisResult {
        let request = VNDetectHorizonRequest()
        
        do {
            try handler.perform([request])
            
            if let observation = request.results?.first as? VNHorizonObservation {
                print("ğŸ” åœ°å¹³çº¿æ£€æµ‹: æˆåŠŸ")
                let angle = Float(observation.angle)
                let transform = "\(observation.transform)"
                return .horizonDetection(angle: angle, transform: transform)
            } else {
                print("âš ï¸ Vision: åœ°å¹³çº¿æ£€æµ‹æœªæ‰¾åˆ°åœ°å¹³çº¿")
            }
        } catch {
            print("âŒ Vision: åœ°å¹³çº¿æ£€æµ‹å¤±è´¥ - \(error.localizedDescription)")
            print("   é”™è¯¯è¯¦æƒ…: \(error)")
        }
        
        return .horizonDetection(angle: nil, transform: nil)
    }
    
    // MARK: - æ‘„å½±å±æ€§æ¨æ–­
    
    private func inferPhotographyAttributes(from visionInfo: PhotoVisionInfo) -> PhotographyAttributes {
        var attributes = PhotographyAttributes()
        
        // åœ°å¹³çº¿ç›¸å…³
        if let angle = visionInfo.horizonAngle {
            attributes.hasHorizon = true
            attributes.horizonTilt = angle
        }
        
        // ä¸»ä½“æ•°é‡
        attributes.subjectCount = visionInfo.saliencyObjects.count
        
        // åœºæ™¯ç±»å‹ï¼ˆæœ€é«˜ç½®ä¿¡åº¦ï¼‰
        if let topScene = visionInfo.sceneClassifications.first {
            attributes.sceneType = topScene.identifier
        }
        
        // æ„å›¾ç±»å‹æ¨æ–­ï¼ˆåŸºäºæ˜¾è‘—æ€§å¯¹è±¡ä½ç½®ï¼‰
        if visionInfo.saliencyObjects.count == 1 {
            let obj = visionInfo.saliencyObjects[0]
            let centerX = obj.boundingBox.midX
            let centerY = obj.boundingBox.midY
            
            // åˆ¤æ–­æ˜¯å¦ç¬¦åˆä¸‰åˆ†æ³•
            let isThirdsX = (0.28...0.38).contains(centerX) || (0.62...0.72).contains(centerX)
            let isThirdsY = (0.28...0.38).contains(centerY) || (0.62...0.72).contains(centerY)
            
            if isThirdsX && isThirdsY {
                attributes.compositionType = "ä¸‰åˆ†æ³•æ„å›¾"
            } else if abs(centerX - 0.5) < 0.1 && abs(centerY - 0.5) < 0.1 {
                attributes.compositionType = "å±…ä¸­æ„å›¾"
            } else {
                attributes.compositionType = "è‡ªç”±æ„å›¾"
            }
        } else if visionInfo.saliencyObjects.count > 1 {
            attributes.compositionType = "å¤šä¸»ä½“æ„å›¾"
        }
        
        return attributes
    }
    
    // MARK: - ç»“æœæ‰“å°
    
    private func printVisionResults(_ visionInfo: PhotoVisionInfo) {
        print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ“¸ Vision è¯†åˆ«ç»“æœæ±‡æ€»")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        // åœºæ™¯è¯†åˆ«
        if !visionInfo.sceneClassifications.isEmpty {
            print("\nğŸï¸  åœºæ™¯è¯†åˆ«ï¼ˆå‰5ä¸ªï¼‰:")
            for (index, scene) in visionInfo.sceneClassifications.prefix(5).enumerated() {
                let bar = progressBar(for: scene.confidence)
                print("   \(index + 1). \(scene.identifier)")
                print("      ç½®ä¿¡åº¦: \(String(format: "%.1f%%", scene.confidence * 100)) \(bar)")
            }
        } else {
            print("\nğŸï¸  åœºæ™¯è¯†åˆ«: æœªè¯†åˆ«åˆ°åœºæ™¯")
        }
        
        // æ˜¾è‘—æ€§åˆ†æ
        if !visionInfo.saliencyObjects.isEmpty {
            print("\nğŸ¯ ä¸»ä½“ä½ç½®è¯†åˆ«:")
            for (index, obj) in visionInfo.saliencyObjects.enumerated() {
                let box = obj.boundingBox
                print("   ä¸»ä½“ \(index + 1):")
                print("      ä½ç½®: x=\(String(format: "%.2f", box.origin.x)), y=\(String(format: "%.2f", box.origin.y))")
                print("      å¤§å°: w=\(String(format: "%.2f", box.width)), h=\(String(format: "%.2f", box.height))")
                print("      ç½®ä¿¡åº¦: \(String(format: "%.1f%%", obj.confidence * 100))")
            }
        } else {
            print("\nğŸ¯ ä¸»ä½“ä½ç½®è¯†åˆ«: æœªæ£€æµ‹åˆ°æ˜æ˜¾ä¸»ä½“")
        }
        
        // å¯¹è±¡æ£€æµ‹
        if !visionInfo.recognizedObjects.isEmpty {
            print("\nğŸ¾ å¯¹è±¡æ£€æµ‹ï¼ˆå‰10ä¸ªï¼‰:")
            for (index, object) in visionInfo.recognizedObjects.prefix(10).enumerated() {
                let bar = progressBar(for: object.confidence)
                print("   \(index + 1). \(object.identifier)")
                print("      ç½®ä¿¡åº¦: \(String(format: "%.1f%%", object.confidence * 100)) \(bar)")
            }
        } else {
            print("\nğŸ¾ å¯¹è±¡æ£€æµ‹: æœªæ£€æµ‹åˆ°å¯¹è±¡ï¼ˆä»…æ”¯æŒåŠ¨ç‰©å’Œäººä½“ï¼‰")
        }
        
        // åœ°å¹³çº¿æ£€æµ‹
        if let angle = visionInfo.horizonAngle {
            let degrees = angle * 180 / .pi
            print("\nğŸ“ åœ°å¹³çº¿æ£€æµ‹:")
            print("   è§’åº¦: \(String(format: "%.2f", angle)) å¼§åº¦ (\(String(format: "%.2f", degrees))Â°)")
            if abs(degrees) < 2 {
                print("   çŠ¶æ€: âœ… æ°´å¹³")
            } else {
                print("   çŠ¶æ€: âš ï¸ å€¾æ–œ \(degrees > 0 ? "å³å€¾" : "å·¦å€¾")")
            }
        } else {
            print("\nğŸ“ åœ°å¹³çº¿æ£€æµ‹: æœªæ£€æµ‹åˆ°åœ°å¹³çº¿")
        }
        
        // æ‘„å½±å±æ€§
        if let attrs = visionInfo.photographyAttributes {
            print("\nğŸ“· æ‘„å½±å±æ€§æ¨æ–­:")
            if let sceneType = attrs.sceneType {
                print("   åœºæ™¯ç±»å‹: \(sceneType)")
            }
            if let compositionType = attrs.compositionType {
                print("   æ„å›¾ç±»å‹: \(compositionType)")
            }
            print("   ä¸»ä½“æ•°é‡: \(attrs.subjectCount)")
            if attrs.hasHorizon {
                print("   åœ°å¹³çº¿: å·²æ£€æµ‹")
            }
        }
        
        print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    /// ç”Ÿæˆç½®ä¿¡åº¦è¿›åº¦æ¡
    private func progressBar(for confidence: Float, length: Int = 20) -> String {
        let filled = Int(confidence * Float(length))
        let empty = length - filled
        return String(repeating: "â–ˆ", count: filled) + String(repeating: "â–‘", count: empty)
    }
}
